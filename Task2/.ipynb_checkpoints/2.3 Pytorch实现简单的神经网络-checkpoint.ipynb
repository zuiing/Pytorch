{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 基础 : 神经网络包nn和优化器optm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入相关包\n",
    "\n",
    "torch.nn是专门为神经网络设计的模块化接口。nn构建于 Autograd之上，可用来定义和运行神经网络。 \n",
    "\n",
    "torch.nn.functional包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数(如ReLU，pool，DropOut等)，这些函数可以放在构造函数中，也可以不放，但是这里建议不放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个网络\n",
    "\n",
    "PyTorch中已经为我们准备好了现成的网络模型，只要继承nn.Module，并实现它的forward方法，PyTorch会根据autograd，自动实现backward函数，在forward函数中可使用任何tensor支持的函数，还可以使用if、for循环、print、log等Python语法，写法和标准的Python写法一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1350, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # nn.Module 子类的函数必须在构造函数中执行符类的构造函数，是直接继承吧\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        # 卷积层 '1'表示输入图片为单通道,\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=3) # 即滤波函数，滤波器可能有6个，即每个像素点会输出6个特征值\n",
    "        # 线性层 输入1350个特征，输出10个特征\n",
    "        self.fc1 = nn.Linear(6*15*15,10)\n",
    "        \n",
    "    # 正向传播\n",
    "    def forward(self,x):\n",
    "        print(x.size()) # 结果：[1,1,32,32]\n",
    "        # 卷积-> 激活-> 池化\n",
    "        x = self.conv1(x) # 根据卷积的尺寸计算公式，计算结果是30(在滤波时，边缘部分的像素丢掉了，因此像素由32变成了30，若不想丢失边缘信息，可以在滤波函数中加上padding=1参数)\n",
    "        x = F.relu(x)\n",
    "        print(x.size()) # 结果： [1,16,30,30]\n",
    "        x = F.max_pool2d(x,(2,2)) # 使用池化层，计算结果是15\n",
    "        x = F.relu(x)\n",
    "        print(x.size()) # 结果：[1,1,15,15]\n",
    "        # reshape -1表示自适应 即压扁操作，把[1,6,15,15]变成[1,1350]\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        print(x.size()) # 这里就是fc1层的输入1350\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "        \n",
    "net = Net() # 创建一个该网络模型的对象并实例化\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的可学习参数通过net.parameters()返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.2847,  0.1401,  0.0162],\n",
      "          [ 0.3194,  0.0886,  0.3128],\n",
      "          [-0.0835,  0.2654, -0.2039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0481, -0.3234,  0.0618],\n",
      "          [-0.0072, -0.1787,  0.0061],\n",
      "          [-0.1867,  0.1865, -0.0200]]],\n",
      "\n",
      "\n",
      "        [[[-0.1980, -0.2004, -0.2673],\n",
      "          [ 0.2029, -0.1595, -0.0415],\n",
      "          [ 0.1300,  0.0912,  0.2223]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1319,  0.0436,  0.2009],\n",
      "          [-0.1062, -0.0762,  0.0309],\n",
      "          [ 0.0024, -0.3134, -0.2790]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0498, -0.3296,  0.1787],\n",
      "          [-0.1005,  0.1278,  0.3112],\n",
      "          [ 0.3127,  0.0987, -0.0116]]],\n",
      "\n",
      "\n",
      "        [[[-0.0543,  0.1213, -0.2781],\n",
      "          [-0.2606,  0.3055, -0.3014],\n",
      "          [-0.1090, -0.2538, -0.2968]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0728, -0.1930, -0.1006, -0.2035, -0.1051,  0.0732],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0027, -0.0258, -0.0062,  ...,  0.0192, -0.0044, -0.0249],\n",
      "        [ 0.0262, -0.0210, -0.0200,  ..., -0.0097, -0.0092,  0.0254],\n",
      "        [ 0.0245, -0.0098,  0.0178,  ..., -0.0175, -0.0244, -0.0190],\n",
      "        ...,\n",
      "        [-0.0105,  0.0003,  0.0082,  ...,  0.0105, -0.0053,  0.0029],\n",
      "        [-0.0020,  0.0151, -0.0223,  ..., -0.0222, -0.0250, -0.0147],\n",
      "        [ 0.0144,  0.0192,  0.0182,  ...,  0.0110,  0.0181,  0.0257]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0025, -0.0053,  0.0107,  0.0068, -0.0050, -0.0097,  0.0067, -0.0105,\n",
      "         0.0251, -0.0081], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameter in net.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net.named_parameters()可同时返回学习的参数及名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight : torch.Size([6, 1, 3, 3])\n",
      "conv1.bias : torch.Size([6])\n",
      "fc1.weight : torch.Size([10, 1350])\n",
      "fc1.bias : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in net.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward()函数的输入输出都是tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32) # 这里对应前面forward()的输入是32\n",
    "out = net(input)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播前要将所有参数的梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.ones(1,10)) # 反向传播的实现是Pytorch自动实现的，只需要调用这个函数即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** torch.nn只支持mini_batches,不支持一次只输入一个样本，即一次必须是一个batch。\n",
    "也就是说，就算我们输入一个样本，也会是对样本进行分批，所以，所有的输入都会增加一个维度，对比下刚才的input，nn中定义为三维，但是我们人工创建时多增加了一个维度，变成了4维，最前面的1即为batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "\n",
    "在nn中Pytorch还预制了常用的损失函数，下面用MSELoss来计算均方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.21556282043457\n"
     ]
    }
   ],
   "source": [
    "y = torch.arange(0,10).view(1,10).float()\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out,y) # loss 是个scalar，可以直接用item()获取它的python类型的数值\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器\n",
    "\n",
    "在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：\n",
    "\n",
    "weight = weight - learning_rate * gradient\n",
    "\n",
    "在torch.optim中实现大多数的优化方法，例如RMSProp、Adam、SGD等，下面使用SGD做个简单的样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "\n",
    "out=net(input) # 这里调用的时候会打印出我们在forword函数中打印的x的大小\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out,y)\n",
    "\n",
    "# 创建一个优化器，SGD只需要调整的参数和学习率\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "# 梯度清零(与net.zero_grad()效果一样))\n",
    "optimizer.zero_grad()\n",
    "loss.backward() # 反向传播\n",
    "\n",
    "# 更新参数\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是忍不住用C++的思想来理解这一切 >_<!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
